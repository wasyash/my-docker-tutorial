{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "406e56c7-2610-47f7-8b0f-168a05586776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-09T12:41:04.393775845Z main ERROR Reconfiguration failed: No configuration found for '4e0e2f2a' at 'null' in 'null'\n",
      "2026-02-09T12:41:05.049738230Z main ERROR Reconfiguration failed: No configuration found for 'Default' at 'null' in 'null'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/02/09 12:41:05 WARN Utils: Your hostname, codespaces-8b0c36, resolves to a loopback address: 127.0.0.1; using 10.0.0.107 instead (on interface eth0)\n",
      "26/02/09 12:41:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/09 12:41:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Dlog4j.configuration=file:log4j.properties\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9260041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet('data/pq/green/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30dfa0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df_green \\\n",
    "    .select('lpep_pickup_datetime', 'PULocationID', 'total_amount') \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59f041f6-b220-4570-a0f3-79f38ea52276",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 23, 13, 10, 15), PULocationID=74, total_amount=44.97),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 20, 15, 9), PULocationID=67, total_amount=33.45),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 15, 20, 23, 41), PULocationID=260, total_amount=8.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 5, 16, 32, 26), PULocationID=82, total_amount=8.3),\n",
       " Row(lpep_pickup_datetime=datetime.datetime(2020, 1, 29, 19, 22, 42), PULocationID=166, total_amount=12.74)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "808c9d93-c0db-4931-b1aa-9acba955c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "start = datetime(year=2020, month=1, day=1)\n",
    "\n",
    "def filter_outliers(row):\n",
    "    return row.lpep_pickup_datetime >= start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a433281-61eb-4d62-a1d3-3f3a0a8ae455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_grouping(row): \n",
    "    hour = row.lpep_pickup_datetime.replace(minute=0, second=0, microsecond=0)\n",
    "    zone = row.PULocationID\n",
    "    key = (hour, zone)\n",
    "    \n",
    "    amount = row.total_amount\n",
    "    count = 1\n",
    "    value = (amount, count)\n",
    "\n",
    "    return (key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b122a374-b4bb-43bd-94a6-09508cc061de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_revenue(left_value, right_value):\n",
    "    left_amount, left_count = left_value\n",
    "    right_amount, right_count = right_value\n",
    "    \n",
    "    output_amount = left_amount + right_amount\n",
    "    output_count = left_count + right_count\n",
    "    \n",
    "    return (output_amount, output_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bed9842-6e58-4d3c-9c18-1e840067225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b10f83cf-e882-4e94-8dbe-c6ece3ae61d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RevenueRow = namedtuple('RevenueRow', ['hour', 'zone', 'revenue', 'count'])\n",
    "def unwrap(row):\n",
    "    return RevenueRow(\n",
    "        hour=row[0][0], \n",
    "        zone=row[0][1],\n",
    "        revenue=row[1][0],\n",
    "        count=row[1][1]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85ad8a54-6425-410e-b2c4-c696765c700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7893d229-a9c0-451b-a062-8791630ac9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_schema = types.StructType([\n",
    "    types.StructField('hour', types.TimestampType(), True),\n",
    "    types.StructField('zone', types.IntegerType(), True),\n",
    "    types.StructField('revenue', types.DoubleType(), True),\n",
    "    types.StructField('count', types.IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "48802d05-24fc-4745-b5fb-e0e7c16de81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = rdd \\\n",
    "    .filter(filter_outliers) \\\n",
    "    .map(prepare_for_grouping) \\\n",
    "    .reduceByKey(calculate_revenue) \\\n",
    "    .map(unwrap) \\\n",
    "    .toDF(result_schema) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8bae3cdc-1e77-4f32-85fb-9788a0d7e75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======================================>                   (2 + 1) / 3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----+------------------+-----+\n",
      "|               hour|zone|           revenue|count|\n",
      "+-------------------+----+------------------+-----+\n",
      "|2020-01-23 13:00:00|  74|1044.0499999999993|   60|\n",
      "|2020-01-15 11:00:00| 179|              50.5|    5|\n",
      "|2020-01-16 08:00:00|  41| 736.1399999999996|   54|\n",
      "|2020-01-31 11:00:00| 260|            126.32|    7|\n",
      "|2020-01-02 08:00:00|  66|            197.69|   10|\n",
      "|2020-01-16 14:00:00|  74| 895.1799999999994|   60|\n",
      "|2020-01-23 12:00:00|  37|             43.35|    2|\n",
      "|2020-01-10 22:00:00|  95| 407.7100000000002|   37|\n",
      "|2020-01-13 19:00:00| 223|213.83000000000004|   19|\n",
      "|2020-01-21 18:00:00| 166| 685.3899999999999|   50|\n",
      "|2020-01-23 15:00:00| 166| 901.6799999999995|   59|\n",
      "|2020-01-07 18:00:00|  25| 554.2900000000001|   37|\n",
      "|2020-01-01 01:00:00| 181| 730.1499999999997|   25|\n",
      "|2020-01-18 07:00:00|  55|              48.3|    1|\n",
      "|2020-01-20 00:00:00|  74|221.33000000000004|   16|\n",
      "|2020-01-11 16:00:00|  65|            936.16|   43|\n",
      "|2020-01-03 11:00:00|  51| 305.9599999999999|   10|\n",
      "|2020-01-11 18:00:00|  42| 527.9200000000002|   35|\n",
      "|2020-01-23 12:00:00|  25|            284.11|   14|\n",
      "|2020-01-30 09:00:00|  22|114.80000000000001|    4|\n",
      "+-------------------+----+------------------+-----+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):                                              \n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 233, in manager\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 87, in worker\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54670f01-430c-46b1-a761-5a4717fd02a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_result.write.parquet('tmp/green-revenue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6573ea3b-1342-4dbd-9be5-e58920e66603",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['VendorID', 'lpep_pickup_datetime', 'PULocationID', 'DOLocationID', 'trip_distance']\n",
    "\n",
    "duration_rdd = df_green \\\n",
    "    .select(columns) \\\n",
    "    .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ab6d93b-c855-41df-a6ae-dad728a5bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "474e1707-1553-4b4f-81b3-8af698298f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "rows = duration_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c83f7405-d1c8-40e0-87e5-9228b9218968",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rows, columns=columns).astype({\"lpep_pickup_datetime\" : str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73191d7a-ccf2-4c9c-969d-ecbaf9698c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VendorID                float64\n",
       "lpep_pickup_datetime        str\n",
       "PULocationID              int64\n",
       "DOLocationID              int64\n",
       "trip_distance           float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1125b55-9b6d-44c5-9638-a4cad71fdead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(df):\n",
    "#     y_pred = model.predict(df)\n",
    "    y_pred = df.trip_distance * 5\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c59f81f8-e534-4388-8435-803233171468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_in_batch(rows):\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    df['VendorID'] = df['VendorID'].fillna(-1)\n",
    "    df = df.astype({\"VendorID\": int, \"lpep_pickup_datetime\" : str})\n",
    "    predictions = model_predict(df)\n",
    "    df['predicted_duration'] = predictions\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f36000ff-1674-475f-b4dd-989eaa7377ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_predicts = duration_rdd \\\n",
    "    .mapPartitions(apply_model_in_batch)\\\n",
    "    .toDF() \\\n",
    "    .drop('Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "845420b2-62d8-48b9-96ea-5f757b996512",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------+------------+-------------+------------------+\n",
      "|VendorID|lpep_pickup_datetime|PULocationID|DOLocationID|trip_distance|predicted_duration|\n",
      "+--------+--------------------+------------+------------+-------------+------------------+\n",
      "|       2| 2020-01-23 13:10:15|          74|         130|        12.77|63.849999999999994|\n",
      "|      -1| 2020-01-20 15:09:00|          67|          39|          8.0|              40.0|\n",
      "|       2| 2020-01-15 20:23:41|         260|         157|         1.27|              6.35|\n",
      "|       2| 2020-01-05 16:32:26|          82|          83|         1.25|              6.25|\n",
      "|       2| 2020-01-29 19:22:42|         166|          42|         1.84| 9.200000000000001|\n",
      "|       2| 2020-01-15 11:07:42|         179|         223|         0.76|               3.8|\n",
      "|       2| 2020-01-16 08:22:29|          41|         237|         3.32|16.599999999999998|\n",
      "|       2| 2020-01-28 17:05:28|          75|         161|         2.21|             11.05|\n",
      "|       1| 2020-01-22 14:51:37|         152|         166|          0.9|               4.5|\n",
      "|       2| 2020-01-31 10:25:04|          75|         234|          6.1|              30.5|\n",
      "|       2| 2020-01-20 15:50:54|          75|          41|         1.74|               8.7|\n",
      "|       2| 2020-01-31 11:35:17|         260|         226|         1.18|5.8999999999999995|\n",
      "|       1| 2020-01-04 20:44:28|         129|         129|          2.2|              11.0|\n",
      "|       2| 2020-01-17 21:47:52|          74|         126|         3.04|              15.2|\n",
      "|       2| 2020-01-21 23:13:47|          61|          61|         0.85|              4.25|\n",
      "|       2| 2020-01-02 08:11:21|          66|         164|         5.06|25.299999999999997|\n",
      "|       2| 2020-01-27 02:59:20|           7|         179|         1.57|7.8500000000000005|\n",
      "|       2| 2020-01-16 14:39:13|          74|         243|          6.8|              34.0|\n",
      "|       2| 2020-01-16 18:42:24|          66|          97|         1.06| 5.300000000000001|\n",
      "|       2| 2020-01-03 09:24:54|          61|         225|         1.23|              6.15|\n",
      "+--------+--------------------+------------+------------+-------------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):                                              \n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 233, in manager\n",
      "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py\", line 87, in worker\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "df_predicts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f12280-d055-4170-ab46-7ed634627fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
